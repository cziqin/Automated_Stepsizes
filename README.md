# Automating Stepsizes for Decentralized Optimization and Learning with Geometric Convergence
Decentralized optimization is a promising paradigm for addressing fundamental challenges in machine learning. However, despite the unprecedented success of existing decentralized optimization and learning methods, the selection of effective stepsizes is still elusive.

We propose an approach that allows individual agents to autonomously adapt their individual stepsizes. 
The effectiveness of the proposed approach is confirmed using the following three typical machine learning applications on benchmark datasets, including logistic regression, matrix factorization, and image classification.
![Introduction](https://github.com/cziqin/Automated_Stepsizes/blob/main/figures/introduction.png)
## ğŸ•µï¸ Outlines
- Installation Tutorial and Preliminaries
- Logistic Regression
- Matrix Factorization
- Training of Convolutional Neural Networks
- Discussions
- License

## ğŸ”§ Installation Tutorial and Preliminaries

### Install Setup
1. Clone this [repository](https://github.com/cziqin/Automated_Stepsizes/tree/main)
2. Download and install [Anaoconda](https://www.anaconda.com) (if you don't have it already)
3. Create a new conda environment with python 3.12
```bash
conda create -n autostep python=3.12
conda activate autostep
```
4. Install any additional packages you need in this environment using conda or pip (tensorflow, pytorch, etc.,)
```sh
pip install -r requiremens.txt
```

### Hardware/computing resources
The experiments were conducted using a system with 32 CPU cores, 31GB of memory, and an NVIDIA GeForce RTX 4090 GPU with 24GB VRAM.

### Repository Structure

```
â”œâ”€â”€ logistic_regression                 # Directory to implement your custom algorithm/trainable/agent
â”‚Â Â  â”œâ”€â”€ custom_random_agent
â”‚Â Â  â”œâ”€â”€ random_policy
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â””â”€â”€ registry.py                     # Register your custom agents here
â”œâ”€â”€ envs
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ framestack.py                   # Example for using custom env wrappers
â”‚Â Â  â”œâ”€â”€ procgen_env_wrapper.py          # Base env used during evaluations (DO NOT EDIT)
â”œâ”€â”€ experiments                         # Directory contaning the config for different experiments
â”‚Â Â  â”œâ”€â”€ impala-baseline.yaml            # Baseline using impala
â”‚Â Â  â”œâ”€â”€ procgen-starter-example.yaml    # Sample experiment config file
â”‚Â Â  â””â”€â”€ random-policy.yaml              # Sample random policy config file
â”œâ”€â”€ models                              # Directory to implement custom models
â”‚Â Â  â”œâ”€â”€ impala_cnn_tf.py
â”‚Â Â  â”œâ”€â”€ impala_cnn_torch.py
â”‚Â Â  â””â”€â”€ my_vision_network.py
â”œâ”€â”€ preprocessors                       # Directory to implement your custom observation wrappers
â”‚Â Â  â”œâ”€â”€ __init__.py                     # Register your preprocessors here
â”‚Â Â  â””â”€â”€ custom_preprocessor.py
â”œâ”€â”€ utils                               # Helper scripts for the competition
â”‚Â Â  â”œâ”€â”€ setup.sh                        # Setup local procgen environment using `conda`
â”‚Â Â  â”œâ”€â”€ submit.sh                       # Submit your solution
â”‚Â Â  â”œâ”€â”€ teardown.sh                     # Remove the existing local procgen environment using `conda`
â”‚Â Â  â”œâ”€â”€ validate_config.py              # Validate the experiment YAML file
â”‚Â Â  â””â”€â”€ loader.py
â”œâ”€â”€ Dockerfile                          # Docker config for your submission environment
â”œâ”€â”€ aicrowd.json                        # Submission config file (required)
â”œâ”€â”€ callbacks.py                        # Custom Callbacks & Custom Metrics
â”œâ”€â”€ requirements.txt                    # These python packages will be installed using `pip`
â”œâ”€â”€ rollout.py                          # Rollout script (DO NOT EDIT)
â”œâ”€â”€ run.sh                              # Entrypoint to your submission
â””â”€â”€ train.py                            # Script to trigger the training using `rllib` (DO NOT EDIT)

```

### Datasets
| Datasets | Download link | Storage Location|
| ------ | ------ | ------|
| Mushrooms | https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/ |`./mushrooms`|
| MovieLens 100k | https://grouplens.org/datasets/movielens/|`./matrix_factorization/data/`|
| CIFAR-10 | https://www.cs.toronto.edu/~kriz/cifar.html |`./Neural_networks/data/`|
| ImageNet | https://academictorrents.com/collection/imagenet-2012 |`./Neural_networks/data/`|

Ensure that each dataset is downloaded and placed in its corresponding folder before running the experiments.

## ğŸ’ª Logistic regression
This experimental code is located in the Logistic_regression folder. To run the main script, use the following command:
```
python Logistic_regression/main.py --test_num 0 --iterations 1000
```
- `--test_num`: Specifies the optimization algorithm to be used:\
`0`:Algorithm 1; `1`: Algorithm 3; `2`:Algorithm 4; `3`: DGM-BB-C; `4`: DGD.
- `--iterations`: sets the number of trianing iterations.
- To run Algorithm S1, please modify the `optimizers.py` file by setting `self.K=1` in the `Algorithm4` class:
```
class Algorithm4(Trainer):
    def __init__(self, *args, **kwargs):
        super(Algorithm4, self).__init__(*args, **kwargs)
        self.name = "Algorithm4"
        self.K = 1  # Change this from 1000 to 1 for Algorithm S1
        self.agent_y = {}
```
- The results will be saved as `.csv` files in the `./results/` directory. 
### Experimental results
![Introduction](https://github.com/cziqin/Automated_Stepsizes/blob/main/figures/mushrooms_png.png)




## ğŸ’ª Matrix factorization
The "MovieLens 100k" dataset used for this experiment is already included in the matrix_factorization folder. To run this experiment, please execute the ``mf.py`` file.

## ğŸ’ª Neural network training
### Cifar 10
The CIFAR-10 experiments used a four-layer CNN, which is provided in the file 'models.py' in the 'Neural_Network' folder.

### ImageNet
The ImageNet experiments used a ResNet-18 architecture, which is provided in the file 'resnet.py' within the 'Neural_Network' folder.

> Note:

## ğŸš€ Discussions
